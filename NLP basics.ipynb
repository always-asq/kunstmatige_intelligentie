{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language processing (NLP)\n",
    "\n",
    "In this notebook some example for natural language processing.\n",
    "\n",
    "Natural Language refers to the way we humans communicate with each other and processing is basically proceeding the data in an understandable form. so we can say that NLP (Natural Language Processing) is a way that helps computers to communicate with humans in their own language.\n",
    "\n",
    "It is one of the broadest fields in research because there is a huge amount of data out there and from that data, a big amount of data is text data. So when there is so much data available so we need some technique threw which we can process the data and retrieve some useful information from it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use NLTK - a python library to help with NLP functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are going to look at:\n",
    "* Tokenization\n",
    "* Stopwords\n",
    "* Stemming\n",
    "* Lemmatizer\n",
    "* WordNet\n",
    "* Part of speech tagging\n",
    "* Bag of Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of dividing the whole text into tokens.\n",
    "\n",
    "It is mainly of two types:\n",
    "\n",
    "* Word Tokenizer (separated by words)\n",
    "* Sentence Tokenizer (separated by sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "example_text = \"Hello there, how are you doing today? The weather is great today. The sky is blue. python is awsome\"\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code\n",
    "First, we are importing `nltk` , in the second line, we are importing our tokenizers `sent_tokenize`,`word_tokenize` from library `nltk.tokenize` , then to use the tokenizer on a text we just need to pass the text as a parameter in the tokenizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "In general stopwords are the words in any language which does not add much meaning to a sentence. In NLP stopwords are those words which are not important in analyzing the data.  \n",
    "Example : he,she,hi,and etc.  \n",
    "Our main task is to remove all the stopwords for the text to do any further processing.\n",
    "\n",
    "There are a total of 179 stopwords in English, using NLTK we can see all the stopwords in English.  \n",
    "We Just need to import `stopwords` from the library `nltk.corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove stopwords for a particular text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = 'he is a good boy. he is very good in coding'\n",
    "text = word_tokenize(text)\n",
    "text_with_no_stopwords = [word for word in text if word not in stopwords.words('english')]\n",
    "text_with_no_stopwords\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.  \n",
    "In simple words, we can say that stemming is the process of removing plural and adjectives from the word.  \n",
    "Example :  \n",
    "loved → love, learning →learn\n",
    "\n",
    "In python, we can implement stemming by using `PorterStemmer`. we can import it from the library `nltk.stem`.\n",
    "\n",
    "**One thing to remember from Stemming is that it works best with single words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()    ## Creating an object for porterstemmer\n",
    "example_words = ['earn',\"earning\",\"earned\",\"earns\"]  ##Example words\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))    ##Using ps object stemming the word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "Lemmatization usually refers to doing things properly with the use of vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.  \n",
    "In simple words lemmatization does the same work as stemming, the difference is that lemmatization returns a meaningful word.  \n",
    "Example:  \n",
    "Stemming  \n",
    "history → histori  \n",
    "Lemmatizing  \n",
    "history → history\n",
    "\n",
    "**It is Mostly used when designing chatbots, Q&A bots, text prediction, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() ## Create object for lemmatizer\n",
    "example_words = ['history','formality','changes']\n",
    "print('Lemmatizer:')\n",
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w))\n",
    "print('\\nStemming:')\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.  \n",
    "We can use `wordnet` for finding synonyms and antonyms.\n",
    "\n",
    "In python, we can import `wordnet` from `nltk.corpus`.\n",
    "\n",
    "Code For Finding Synonym and antonym for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []   ## Creaing an empty list for all the synonyms\n",
    "antonyms =[]    ## Creaing an empty list for all the antonyms\n",
    "for syn in wordnet.synsets(\"happy\"): ## Giving word \n",
    "    for i in syn.lemmas():        ## Finding the lemma,matching \n",
    "        synonyms.append(i.name())  ## appending all the synonyms       \n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name()) ## antonyms\n",
    "print(set(synonyms)) ## Converting them into set for unique values\n",
    "print(set(antonyms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "It is a process of converting a sentence to forms — a list of words, a list of tuples (where each tuple is having a form (word, tag)). The tag in the case is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of Speech Tag List**\n",
    "\n",
    "> CC coordinating conjunction  \n",
    " CD cardinal digit  \n",
    " DT determiner  \n",
    " EX existential there (like: “there is” … think of it like “there”)  \n",
    " FW foreign word  \n",
    " IN preposition/subordinating conjunction  \n",
    " JJ adjective ‘big’  \n",
    " JJR adjective, comparative ‘bigger’  \n",
    " JJS adjective, superlative ‘biggest’  \n",
    " LS list marker 1)  \n",
    " MD modal could, will  \n",
    " NN noun, singular ‘desk’  \n",
    " NNS noun plural ‘desks’  \n",
    " NNP proper noun, singular ‘Harrison’  \n",
    " NNPS proper noun, plural ‘Americans’  \n",
    " PDT predeterminer ‘all the kids’  \n",
    " POS possessive ending parent’s  \n",
    " PRP personal pronoun I, he, she  \n",
    " PRP possessive pronoun my, his, hers  \n",
    " RB adverb very, silently,  \n",
    " RBR adverb, comparative better  \n",
    " RBS adverb, superlative best  \n",
    " RP particle give up  \n",
    " TO to go ‘to’ the store.  \n",
    " UH interjection errrrrrrrm  \n",
    " VB verb, base form take  \n",
    " VBD verb, past tense took  \n",
    " VBG verb, gerund/present participle taking  \n",
    " VBN verb, past participle taken  \n",
    " VBP verb, sing. present, non-3d take  \n",
    " VBZ verb, 3rd person sing. present takes  \n",
    " WDT wh-determiner which  \n",
    " WP wh-pronoun who, what  \n",
    " WP possessive wh-pronoun whose  \n",
    " WRB wh-abverb where, when  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, we can do pos tagging using `nltk.pos_tag`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''\n",
    "An sincerity so extremity he additions. Her yet there truth merit. Mrs all projecting favourable now unpleasing. Son law garden chatty temper. Oh children provided to mr elegance marriage strongly. Off can admiration prosperous now devonshire diminution law.\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(sample_text)\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "Till now we have understood about tokenizing, stemming, and lemmatizing. all of these are the part of the text cleaning, now after cleaning the text we need to convert the text into some kind of numerical representation called vectors so that we can feed the data to a machine learning model for further processing.\n",
    "\n",
    "For converting the data into vectors we make use of some predefined libraries in python.\n",
    "\n",
    "Let’s see how vector representation works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sent1 = he is a good boy\n",
    "sent2 = she is a good girl\n",
    "sent3 = boy and girl are good \n",
    "        |\n",
    "        |\n",
    "  After removal of stopwords , lematization or stemming\n",
    "sent1 = good boy\n",
    "sent2 = good girl\n",
    "sent3 = boy girl good  \n",
    "        | ### Now we will calculate the frequency for each word by\n",
    "        |     calculating the occurrence of each word\n",
    "word  frequency\n",
    "good     3\n",
    "boy      2\n",
    "girl     2\n",
    "         | ## Then according to their occurrence we assign o or 1 \n",
    "         |    according to their occurrence in the sentence\n",
    "         | ## 1 for present and 0 fot not present\n",
    "         f1  f2   f3\n",
    "        girl good boy   \n",
    "sent1    0    1    1     \n",
    "sent2    1    0    1\n",
    "sent3    1    1    1\n",
    "### After this we pass the vector form to machine learning model`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process can be done using a `CountVectorizer` in python, we can import the same from `sklearn.feature_extraction.text`.\n",
    "\n",
    "CODE to implement `CountVectorizer` In python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sent = pd.DataFrame(['he is a good boy', 'she is a good girl', 'boy and girl are good'],columns=['text'])\n",
    "corpus = []\n",
    "for i in range(0,3):\n",
    "    words = sent['text'][i]\n",
    "    words  = word_tokenize(words)\n",
    "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    text = ' '.join(texts)\n",
    "    corpus.append(text)\n",
    "print(corpus)   #### Cleaned Data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X  ## Vectorize Form "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
